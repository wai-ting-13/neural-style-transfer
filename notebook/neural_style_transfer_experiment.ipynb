{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xi4N8oajTyS"
   },
   "source": [
    "# Jupyter Note - Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KfkGKJ-_Ypf"
   },
   "source": [
    "# Google Drive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current working directory\n",
    "YOUR_WORKING_DIR_GDRIVE=\"MyDrive/Colab/neural-style-transfer/notebook\"\n",
    "\n",
    "import os;\n",
    "os.chdir(f\"/content/gdrive/{YOUR_WORKING_DIR_GDRIVE}\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5TdH1gj_e1B"
   },
   "source": [
    "# Import packages & Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1756968330273,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "f76bONpYjTyW",
    "outputId": "6547cdae-35fa-4d74-ec30-0196bc863e27"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "dvc = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print Device and Library Version\n",
    "print(f\"Using device: {dvc}\")\n",
    "print(f\"Python version={sys.version}\")\n",
    "print(f\"torch version={torch.__version__}\")\n",
    "print(f\"torchvision version={torchvision.__version__}\")\n",
    "print(f\"accelerate version={accelerate.__version__}\")\n",
    "print(f\"matplotlib version={matplotlib.__version__}\")\n",
    "\n",
    "# Create Directory\n",
    "INPUT_PATH=\"./inputs\"\n",
    "OUTPUT_PATH=\"./outputs\"\n",
    "os.makedirs(INPUT_PATH, exist_ok=True) # Input Directory\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True) # Output Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQSjO48wjTyY"
   },
   "source": [
    "# Define the model aritechture for our task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5qYoLOHjTyY"
   },
   "source": [
    "## Import Pretrained VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWNXE34Ad1rU"
   },
   "outputs": [],
   "source": [
    "vgg16_model = torchvision.models.vgg16(weights=torchvision.models.vgg.VGG16_Weights.DEFAULT)\n",
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9171,
     "status": "ok",
     "timestamp": 1756968294107,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "e3WsZWyTjTyY",
    "outputId": "a2d6a33b-0a51-4281-8bc3-fe5e8de8556e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vgg19_model = torchvision.models.vgg19(weights=torchvision.models.vgg.VGG19_Weights.DEFAULT)\n",
    "print(vgg19_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ovzpAyyjTyZ"
   },
   "source": [
    "## Build Our Own Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756968297223,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "mF_G5IlyWQCP"
   },
   "outputs": [],
   "source": [
    "\"\"\"Model Definition\"\"\"\n",
    "class NSTNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_extractor : nn.Module,\n",
    "        selected_style_layernames : list[str],\n",
    "        selected_content_layernames : list[str],\n",
    "        use_avgpool : bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Get Indices\n",
    "        self.style_loss_indices = [i for i, _ in enumerate(selected_style_layernames)]\n",
    "        self.content_loss_indices = [i for i, name in enumerate(selected_content_layernames) if name in selected_style_layernames]\n",
    "\n",
    "        # Define Normalisation Function\n",
    "        self.normalise = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "        slices : list[nn.Sequencial] = []\n",
    "        slice = nn.Sequential()\n",
    "\n",
    "        i = 0;\n",
    "        for layer in feature_extractor.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                i += 1\n",
    "                name = 'conv_{}'.format(i)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                name = 'relu_{}'.format(i)\n",
    "                layer = nn.ReLU(inplace=False)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                name = 'pool_{}'.format(i)\n",
    "                layer = nn.AvgPool2d(layer.kernel_size, layer.stride, layer.padding) if use_avgpool else layer\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                name = 'bn_{}'.format(i)\n",
    "            else:\n",
    "                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "            slice.add_module(name, layer)\n",
    "            if name in selected_style_layernames:\n",
    "                slices.append(slice)\n",
    "                slice = nn.Sequential()\n",
    "\n",
    "        self.extractor = nn.Sequential()\n",
    "        for i, slice in enumerate(slices,1):\n",
    "            self.extractor.add_module(f\"slice_{i}\", slice)\n",
    "\n",
    "    def forward(self, x) -> list[torch.Tensor]:\n",
    "        x = self.normalise(x)\n",
    "        feature_maps : list[torch.Tensor] = []\n",
    "        for slice in self.extractor.children():\n",
    "            x = slice(x)\n",
    "            feature_maps.append(x)\n",
    "        return feature_maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXmfmuGAFHJf"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1756968301032,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "b3mbm87skyPV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Information:\n",
      "NSTNetwork(\n",
      "  (normalise): Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "  (extractor): Sequential(\n",
      "    (slice_1): Sequential(\n",
      "      (conv_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (slice_2): Sequential(\n",
      "      (relu_1): ReLU()\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (slice_3): Sequential(\n",
      "      (relu_2): ReLU()\n",
      "      (pool_2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (slice_4): Sequential(\n",
      "      (relu_3): ReLU()\n",
      "      (conv_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (slice_5): Sequential(\n",
      "      (relu_4): ReLU()\n",
      "      (pool_4): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      (conv_5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:6: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "/var/folders/0h/6kdvvxs11jl4w17qg3tk45kw0000gn/T/ipykernel_1274/4066387771.py:6: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(\n"
     ]
    }
   ],
   "source": [
    "def create_model() -> nn.Module:\n",
    "    style_layer_names=[\"conv_1\", \"conv_2\", \"conv_3\", \"conv_4\", \"conv_5\"]\n",
    "    content_layer_names=[\"conv_4\", \"conv_6\"]\n",
    "\n",
    "    # Assert all `content_layer_names` are subset of s`tyle_layer_names`\n",
    "    assert \\\n",
    "        all(map(lambda n: n in style_layer_names, content_layer_names)), \\\n",
    "        \"One of `content_layer_names` is not in `style_layer_names`\"\n",
    "    \n",
    "\n",
    "    # Define Our Model\n",
    "    net = NSTNetwork(\n",
    "        feature_extractor=vgg19_model.features,\n",
    "        selected_style_layernames=style_layer_names,\n",
    "        selected_content_layernames=content_layer_names,\n",
    "        use_avgpool=True\n",
    "    )\n",
    "\n",
    "    # Disable Gradient and Turn Model to Evaluation Model\n",
    "    net.requires_grad_(False)\n",
    "    net.eval()\n",
    "    net.to(dvc)\n",
    "\n",
    "    return net\n",
    "\n",
    "net = create_model()\n",
    "print(\"Neural Network Information:\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzwoEe6T0F_5"
   },
   "source": [
    "## Import Content and Style Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 1289,
     "status": "ok",
     "timestamp": 1756968615847,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "WRoqHIxnjTyW",
    "outputId": "5d0ac23c-900a-4700-d65b-e977edab2020"
   },
   "outputs": [],
   "source": [
    "BIG_DIM=512\n",
    "SMALL_DIM=128\n",
    "image_dimension = BIG_DIM if torch.cuda.is_available() else SMALL_DIM\n",
    "\n",
    "def image_to_tensor(image_filepath : str, image_dimension : int = SMALL_DIM) -> torch.Tensor:\n",
    "    img = Image.open(image_filepath).convert('RGB')\n",
    "\n",
    "    print(f\"Original image size: {img.size}\")\n",
    "\n",
    "    # display image to check\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10, 6))\n",
    "    axs[0].set_title(f\"{image_filepath}\")\n",
    "    axs[0].imshow(img)\n",
    "\n",
    "    # Central-crop the image if it is not square\n",
    "    if img.height != img.width:\n",
    "        width, height = img.size\n",
    "        min_dim = min(width, height)\n",
    "        left = (width - min_dim) / 2\n",
    "        top = (height - min_dim) / 2\n",
    "        right = (width + min_dim) / 2\n",
    "        bottom = (height + min_dim) / 2\n",
    "        box = (left, top, right, bottom)\n",
    "        img = img.crop(box)\n",
    "\n",
    "    # Scale-up image if it is too small\n",
    "    if img.height < image_dimension or img.width < image_dimension:\n",
    "      scaling_factor = image_dimension / max(img.size)\n",
    "\n",
    "      new_width = int(img.width * scaling_factor)\n",
    "      new_height = int(img.height * scaling_factor)\n",
    "\n",
    "      img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    print(f\"New image size: {img.size}\")\n",
    "\n",
    "    torch_transformation = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_dimension),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    img = torch_transformation(img).unsqueeze(0)\n",
    "\n",
    "    # Display Processed Image, Sub plt\n",
    "    axs[1].set_title(f\"{image_filepath} Processed\")\n",
    "    axs[1].imshow(img.squeeze(0).cpu().detach().numpy().transpose(1,2,0))\n",
    "\n",
    "    return img.to(torch.float)\n",
    "\n",
    "style_image = image_to_tensor(f\"{INPUT_PATH}/style-4.jpg\", image_dimension).to(dvc).detach()\n",
    "content_image = image_to_tensor(f\"{INPUT_PATH}/content-1.jpg\", image_dimension).to(dvc).detach()\n",
    "print(f\"style_image.shape: {style_image.shape}\")\n",
    "print(f\"content_image.shape: {content_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFHejcX_0arD"
   },
   "source": [
    "## Define input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1756968617103,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "G9mvJslxjTyZ",
    "outputId": "99cdba30-a18b-453a-ac51-9c26c1117604",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize as the content image\n",
    "# ip_image = content_image.clone().to(dvc)\n",
    "\n",
    "# initialize as random noise:\n",
    "ip_image = torch.randn(content_image.data.size(), device=dvc)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(ip_image.squeeze(0).cpu().detach().numpy().transpose(1,2,0).clip(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVzt1GDH0iJ3"
   },
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2huaq02A5b-"
   },
   "source": [
    "### Define Functions and Variables before inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1756968356725,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "RNyGjXDqjTyX"
   },
   "outputs": [],
   "source": [
    "# Define gram matrix\n",
    "def gram_matrix(ip : torch.Tensor) -> torch.Tensor:\n",
    "    num_batch, num_channels, height, width = ip.size()\n",
    "    feats = ip.view(num_batch * num_channels, width * height)\n",
    "    gram_mat = torch.mm(feats, feats.t())\n",
    "    return gram_mat.div(num_batch * num_channels * width * height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV81v0KPA-wz"
   },
   "source": [
    "### Do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mode(\n",
    "        \n",
    ") -> nn.Module:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1sxqw60sEKXAeAJmRj1_XMrVQlMaqwCkH"
    },
    "executionInfo": {
     "elapsed": 191485,
     "status": "ok",
     "timestamp": 1756968820210,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "LusIJf2RjTya",
    "outputId": "9fb1e54c-d781-4d3b-8b4c-47b1345698ad",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_epochs= 2000\n",
    "loss_checkpoint_step = 50\n",
    "image_checkpoint_step = 200\n",
    "\n",
    "epoch_style_losses = []\n",
    "epoch_content_losses = []\n",
    "\n",
    "ip_image = Variable(ip_image, requires_grad=True)\n",
    "\n",
    "wt_style=1e5\n",
    "wt_content=1\n",
    "\n",
    "opt = optim.LBFGS([ip_image], lr=1)\n",
    "\n",
    "for curr_epoch in range(1, num_epochs+1):\n",
    "    ip_image.data.clamp_(0, 1)\n",
    "    opt.zero_grad()\n",
    "\n",
    "    epoch_style_loss = 0\n",
    "    epoch_content_loss = 0\n",
    "    epoch_pixel_loss = 0\n",
    "\n",
    "    x = ip_image\n",
    "    yc = content_image.detach()\n",
    "    ys = style_image.detach()\n",
    "\n",
    "    feature_maps_x = net(x)\n",
    "    feature_maps_yc = net(yc)\n",
    "    feature_maps_ys = net(ys)\n",
    "\n",
    "    for i,(f_x,f_yc,f_ys) in enumerate(zip(feature_maps_x,feature_maps_yc,feature_maps_ys)):\n",
    "        if i in net.style_loss_indices:\n",
    "            epoch_style_loss += F.mse_loss(gram_matrix(f_x), gram_matrix(f_ys.detach()).detach())\n",
    "        if i in net.content_loss_indices:\n",
    "            epoch_content_loss += F.mse_loss(f_x, f_yc.detach())\n",
    "\n",
    "    epoch_style_loss *= wt_style\n",
    "    epoch_content_loss *= wt_content\n",
    "\n",
    "    total_loss = epoch_style_loss + epoch_content_loss\n",
    "    accelerator.backward(total_loss)\n",
    "\n",
    "    def closure() -> torch.Tensor:\n",
    "        return total_loss\n",
    "\n",
    "    if curr_epoch % loss_checkpoint_step == 0:\n",
    "        epoch_style_losses += [epoch_style_loss.cpu().detach().numpy()]\n",
    "        epoch_content_losses += [epoch_content_loss.cpu().detach().numpy()]\n",
    "        print(f\"epoch number {curr_epoch}\")\n",
    "        print(f\"style loss = {epoch_style_loss}, content loss = {epoch_content_loss}\")\n",
    "\n",
    "    if curr_epoch % image_checkpoint_step == 0:\n",
    "        display_image = ip_image.data.clamp_(0, 1).squeeze(0).cpu().detach()\n",
    "        plt.figure()\n",
    "        plt.title(f\"epoch number {curr_epoch}\")\n",
    "        plt.imshow(display_image.numpy().transpose(1,2,0))\n",
    "        plt.show()\n",
    "        torchvision.utils.save_image(\n",
    "            display_image,\n",
    "            f\"{OUTPUT_PATH}/image_{curr_epoch}.jpg\"\n",
    "        )\n",
    "\n",
    "    opt.step(closure=closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1k0EMCJykLA"
   },
   "source": [
    "## Plot Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1756968588386,
     "user": {
      "displayName": "Wai Ting Hon",
      "userId": "18312159932542596713"
     },
     "user_tz": -480
    },
    "id": "ELHU23dcjTya",
    "outputId": "97ee55a3-f744-4e3d-8053-d5effcc8764b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(loss_checkpoint_step, num_epochs+1, loss_checkpoint_step), epoch_style_losses, label='style_loss');\n",
    "plt.plot(range(loss_checkpoint_step, num_epochs+1, loss_checkpoint_step), epoch_content_losses, label='content_loss');\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pytorch-test-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
